Why REST services should be stateless	Because it enables the fault tolerance and scalability: requests from the same client can be processed on different servers.
What requests are cacheable and how can they be implemented	GET requests are cacheable. It can be defined by adding an HTTP header Cache-Control
Why events can be used for pluggable architecture	Events provide a possibility to attach additional functionality to any part of the application. Event listeners make possible to avoid changing the core part of application. Events allow for the decoupling of components in a system, separating the ownership of data by domain. This enables the addition of new functionality through a pluggable kernel and allows interested parties to be notified of state changes by subscribing to published events.
How to make event listeners to work on separate threads	By default, the event listeners are working on the main thread. You can define bean ApplicationEventMulticaster in @Configuration to allow to use multiple threads. It allows to run several event listeners in parallel and takes the threads from the thread pool, making threads reusable.
What are the use cases for using ETag	ETag is used when we retrieve the same data and need to know if it was changed.  If it was not, we will get HTTP status 304 (Not Modified).
Why does an ETag value represent a hash value of the object	That's because the data can be retrieved in many ways, but if the hash has not changed,  we suppose that data are not changed as well. An ETag value represents a hash value of the object because it is a value that represents the state of an entity. Typically, the ETag value is a hash of the content, a hash of the last modification timestamp, or just a revision number1. The server may attach an ETag HTTP header to the response when requesting a resource, such ETag would represent the state of the resource
Why does using ETag may increase the application performance	Because the server do not pass the whole data to the client if it was not changed  (and therefore server has to transfer less data)
How can we use ETags in a Spring application	We need to add ShallowEtagHeaderFilter to generate ETags on the server-side and perform GET requests  on the client-side with HTTP header If-None-Match: ETag.
Can you provide some examples of when REST events are useful	In Spring Data REST, REST events can define additional behaviour for processing the entity,  it can be used for validation of entity before it is saved (event handler should throw an exception if  validation is failed, and it will prevent data modification in the database).
Is it possible to use REST events with a REST controller	No, REST events can be used only Data REST, with auto-generated REST controller , and allows to define some behaviour in addition to the default.
Why should we use HATEOAS? What are the use cases	 HATEOAS allows us to provide auto-generated documentation of the REST service: we get not just data, but also some "where to go" links, like on the web page.HATEOAS gives a higher level of abstraction: we are able not to rely on the exact hard-coded URLs, but instead, it can be auto-generated (point to the different servers).HATEOAS can provide some standard links like "self" to get the actual data (re-read), or "details" to get more detailed data.Some JSON data can be replaced by the link to the additional data, we can omit the complete address of the person, and add link "/address" to get the detailed information if necessary.
What do we need to add HATEOAS links	We need to implement a RepresentationModelAssembler which includes 2 methods: toModel() which converts entity to EntityModel and adds a link to the single entity, and toCollectionModel() which converts a list of entities to the Collection of EntityModel with links attached to the whole collection.
Why RestTemplate is about to be deprecated? Should we avoid it	It is outdated because it doesn't support reactive programming, and the Spring team recommends using the more universal and contemporary WebClient. However, many projects are still using RestTemplate and most probably it won't be deprecated soon.
Why do we need to call method block() in WebClient	Since WebClient has a reactive interface, we need to call block() to wait until the data will be retrieved. Otherwise, we'd get the unblocking behaviour and will need the reactive operators to process it.
What is BodyExtractors in WebClient, and when should we use it	BodyExtractors allows deserializing the received data (typically JSON).  We should use it to read and process the response from the server.
Why do we need methods BodyExtractors.toMono() and BodyExtractors.toFlux()? What parameter do they take	These methods are needed to read a server response and to convert a single JSON object with BodyExtractors.toMono() or JSON array with BodyExtractors.toFlux()  to a Java object or collection. As a parameter, it takes the class of the data which we should retrieve,  like Person.class: BodyExtractors.toMono(Person.class) will deserialize a JSON to an object of class Mono<Person>
Why is it recommended to use Open Feign	is working much faster than RestTemplate and needs much less code. It allows to define only  the interface of the REST client for efficient use, the same way as it is done Data. when you want to interact with external systems and make use of external APIs. The framework can be easily integrated with Spring Boot, enabling you to rapidly develop and test your systems with external APIs. This can save time and effort compared to manually writing and maintaining web service clients.
What is Open API and which versions of Open API can be used	Open API is a standard for documenting REST API. It can be represented as YAML or JSON. There are  2 widely used versions of Open API: 2 and 3, and Swagger supports both.
What is Swagger and what tools does it provide	Swagger is the most popular set of tools to work with Open API. Swagger UI is used to visually represent Open API to make it convenient for the end-users (usually frontend developers).  Swagger Editor is used to create and edit Open API documents (it is represented as YAML).  Swagger Codegen allows generating a server and a client source code based on Open API specification.
What is SpringFox and when should we use it	It's a Java library that allows to automatically generate Open API specifications from the source code of Spring REST services. It generates the specification (contract) based on your code and also deploys the Swagger UI client with your application, allowing you to immediately test your REST API
What is the difference between Design First and Code First approach? Which tools can be used to implement these approaches	Design First means that first of all analytics are creating REST API specifications, and only then the server is implemented.  Code First means that the server is created first, and then the REST API specifications are generated from the server.  For Design First, we can use Swagger Editor to create specifications + Swagger Codegen to generate the code.  For Code First, we can use SpringFox to generate the documentation from the server.
How can we configure SpringFox to customize the REST documentation	We can use @Api and @ApiOperation annotations to rename or describe the REST operations as it is described in Open API  specification. Also, we can filter out some endpoints by their URLs or by the HTTP method if we don't want them  to be present in the external API. Also, we can define meta-information like authors, license, etc.
Why do we need to limit data passed to the client-side	It is necessary because otherwise, we may pass too much data to the client (in the worst case the whole tree of objects will be serialized to JSON and passed to the client). Also, some data like passwords should be always omitted. There are limits to the amount of data that can be stored using client-side storage APIs, and these limits vary depending on the browser and possibly based on user settings1. By only sending necessary data to the client-side, you can reduce the amount of data that needs to be stored and processed, resulting in faster load times and a more responsive application.
What is the DTO pattern and why should we use it	DTO is Data Transfer Object - the most powerful way to prepare data that should be passed to the client.  However, it needs a lot of extra work to perform converting.
What is nullifying fields approach	Nullifying fields means that we set some properties of the object which will be serialized to JSON to null. Typically, it is performed in the REST controller and doesn't change the database - we just exclude some data from serialization.
What Jackson annotation can be used to limit the data passed to the client and what is a drawback of this approach	Jackson has the annotation @JsonIgnore which allows excluding some property from serialization. The drawback is that @JsonIgnore is not flexible, i.e. it excludes the property forever. Other approaches, like nullifying fields and DTO are more flexible.
What are projections, how to apply them and when can they be used	Projections and excerpts are working only Data REST and can be used with Spring Data repositories. They allow creating subsets of data to be serialized. This is achieved by adding projection parameters  to the URL with the name defined with the @Projection annotation. Projections can be useful when you want to optimize performance by reducing the amount of data that needs to be retrieved from the database. They can also help to simplify your code by allowing you to work with objects that contain only the data you need.
What are the excerpts, how to apply them and when can they be used	Excerpts are like a default projection for the Spring Data repository. However, it is applied [only] to the collections, [not] to single entities. For a [single entity], we have to use the projection parameter.
When and why should we use MapStruct? What is the advantage of using MapStruct	MapStruct automates the generation of DTO for an entity. The advantage of MapStruct is that it allows avoiding the boilerplate code  and defining only non-trivial mappings of Entity to DTO and DTO to Entity.
How can a Jackson serializer be used to customize the serialization to JSON? What is the benefit of using it	A Jackson serializer is a [low-level] approach to customize serialization. We can use @JsonSerialize and @JsonDeserialize to apply the custom serializer. The benefit is that we can avoid adding a DTO layer, and we can provide some common rules for serialization  (the field with the name "password" shouldn't be serialized).
What are the advantages of using the Java Bean Validation framework	 It is a JEE standard (Spring uses Hibernate Validator as implementation) 2) It provides a declarative approach, which makes the code easier to read,  because data constraints are defined with annotations 3) It is customizable, we can create our own validators 4) Standard validators can be propagated to database constraints. Another advantage is that the framework is extendable and customizable, allowing you to define your own custom validation rules if needed. It is integrated with many popular ORMs and the checks are called automatically before changes are saved to a database
How to create a custom validator for the Java Bean Validation framework	 Create the annotation with the name of validator 2) Create a class implementing ConstraintValidator having the method boolean isValid()
How to use an entity validator in a REST controller	 Add the annotation @Valid to @RequestBody 2) Provide a handler for MethodArgumentNotValidException, returning JSON if the validation is failed 3) Put a handler to the REST controller which extends ResponseEntityExceptionHandler and is marked  as @ContollerAdvice - this way you will make it to handle all exceptions of this kind
How to use a validator with request parameters	 Add validation annotations to the controller method parameters, like @PathVariable @Min(1) Long id 2) Add the annotation @Validated to the REST controller class 3) Provide a handler for ConstraintViolationException using method marked with @ExceptionHandler 4) Put the handler to the REST controller marked as @ContollerAdvice - this way you will make it handle all exceptions of this kind
What are the ACID properties of transactions	Atomicity means that either all operations in a transaction are committed, or all are rolled back. Consistency means that the database is in a consistent state after the transaction commits or rolls back. Isolation means that the transaction doesn't interact with the effects of other transactions. Durability means that if the transaction is committed, the changes cannot disappear  (this is achieved by first writing to the transactional log, which is applied again in case of any failure). These properties ensure that transactions are processed reliably and help maintain the integrity of the data in the database.
What is a PlatformTransactionManager? What are the possible implementations of this manager	PlatformTransactionManager is a central interface for working with transactions. It is used in declarative transaction management implicitly and can be used for programmatic transaction management by injecting its instance. It has only 3 methods: getTransaction(), commit() and rollback(). There are various implementations of this interface, and the most frequently used are DataSourceTransactionManager for plain JDBC, JpaTransactionManager to be used with JPA, HibernateTransactionManager for Hibernate, and JmsTransactionManager for working with message brokers.
What is a TransactionDefinition? What parameters does it have	TransactionDefinition is an interface used to configure a transactional method. TransactionDefinition defines 4 properties: isolation, propagation, timeout, and read-only status.
Why should we set a timeout for a transaction? What is the default timeout	We should set the timeout to avoid too long transactions. The unit of timeout is in milliseconds. If the request is not completed during this time, we'll get TransactionException (transaction time expired). Setting timeout might be useful for rolling back long-running database queries. The default value is -1 (no limit).
Why should we set readonly to true	This attribute is the hint for the JPA provider (Hibernate) and JDBC driver  that we are not going to write to DB, which allows performing some additional optimizations. The result is that insert/update queries will not be executed. Also, if you read data and update data, these updates will not be persisted on commit. This means that Hibernate will not check if any changes have been made to the entities and will not issue any update statements to the database. This can be useful when you know that you will not be making any changes to the selected entities and want to optimize performance by avoiding unnecessary dirty checking.
Why do we need a transaction for reading data	 Results consistency: when we want to run more than one SELECT,  and we want the results to be consistent, we should do it in a read-only transaction.If we want to be sure that no attempt to write data to DB will happen.
What is the READ UNCOMMITTED isolation level	READ UNCOMMITTED imposes a minimal number of locks.  It prevents only lost updates - when data are changed by one transaction, the same data cannot be changed by another transaction until commit or rollback.  In the same time, it may allow dirty reads, unrepeatable reads, and phantom reads.
When should you use READ UNCOMMITTED isolation level	It can be used if we need a maximal performance and may consider acceptable the dirty reads and unrepeatable reads. * If we are running aggregations on many rows: the average of one million values will not change significantly * If we are reading local data, that no one else is modifying * If we are reading the old data, that no one is supposed to modify anymore
What is the READ COMMITTED isolation level	This level prevents reading not committed data (dirty reads).  If we update something in the transaction, this data cannot be read by other transactions.  It is achieved by acquiring a lock when we change data. This prevents dirty reads, where a transaction reads data that has been modified by another transaction but not yet committed. However, data can still be changed by other transactions between individual statements within the current transaction, resulting in nonrepeatable reads or phantom data.
When should you use the READ COMMITTED isolation level	It is the default isolation level in many databases (Oracle, PostgreSQL, etc.).  It provides a good balance between performance and isolation.  Unrepeatable or phantom reads may occur on this level. READ COMMITTED is used to prevent dirty reads. It ensures that a transaction only sees data that has been committed by other transactions before the query began and will not see uncommitted data or changes committed during query execution by concurrent transactions.
What is the REPEATABLE READ isolation level	It is an isolation level that prevents dirty reads and unrepeatable reads.  If a transaction is reading some data, this data cannot be changed by other transactions.  It is achieved by acquiring a lock when we are reading data.  The drawback is that we get a lot of locks: we lock everything we look at. 
What is the SERIALIZABLE isolation level	SERIALIZABLE is a level that provides complete isolation.  It prevents all possible problems, including phantom reads.  However, it comes at a cost of too much locking, which makes the database work slower.
What is a lost update problem	The lost update problem is a situation when a database is changed by multiple users and the changes of one user are lost.  if the first user is reading some data, for example A=1, and then the second user is also reading the same value, A=1.  Then the first user sets A=A+1 and saves its value 2 to the database.  The second user does the same: calculates A=A+1 and saves value 2 to the database.  As a result, we had 2 increments: +1 by the first user and +1 by the second user.  However, the database value is still 2, which means that the second user overwrites changes made by the first user.  We have got a lost update problem: changes of the first user are lost.
What is a dirty read problem	Dirty read is reading not consistent data or data from the middle of work of some transaction (dirty data).  It happens when one transaction is changing data but does not commit it, and another transaction is reading data that is not committed yet.  It can be solved by preventing the reading of any changed data (acquiring a lock from reading).  It is solved using the READ COMMITTED isolation level.
What is an unrepeatable read problem	A problem when we read data in the transaction, then after some time read the same data in the same transaction - and get different results.  It can be resolved by preventing the change of the data which was read (by acquiring a lock when we read data).  It is solved using the REPEATABLE READ isolation level. An unrepeatable read occurs when a transaction reads the same row twice and sees different data each time. This can happen when another transaction updates the row between the two reads.
What is a phantom read problem	A problem when we perform several same SELECT ... FROM ... queries in the transaction but get different results.  It happens because other transactions may add new rows (phantoms), and query results may change.  It is solved at the SERIALIZABLE isolation level by acquiring a lock for the whole table. A phantom read occurs when a transaction executes a query twice and it gets a different number of rows in the result set each time. This can happen when a second transaction inserts or deletes rows that match the WHERE clause of the query executed by the first transaction between its two executions.
When should you use the SERIALIZABLE isolation level	If we want to be sure that we have complete isolation of transactions, we should use the SERIALIZABLE isolation level.  Using this level will make the database work slower because of many locks.
What is a transaction propagation	Transaction propagation defines what will happen when one transactional method is calling another one.  Should it reuse the same transaction, or start a new transaction, or prevent running in a transaction, or require a transaction	 - What is a Required transaction propagation and when should it be used	Required is a default transaction propagation for @Transactional annotation.  It should be used when we need a transaction for the method (we always need it if we change any data),  but we want to be able to call the method both from a transactional method or from a non-transactional method.
What is RequiresNew transaction propagation	A method means that it always needs a new transaction.  If the method is called from a non-transactional method, it starts a new transaction.  If a method is called from a transactional method, it suspends|commits a running transaction and creates a new transaction. within its own transaction, independent of any other transactions that may be active at the time.
When should you use RequiresNew transaction propagation	The behavior of violates the atomic principle: it may happen that part of the transaction will roll back, while the transaction will commit, or vice versa.  If transactional methodA() is calling methodB() marked as RequiresNew, methodB() may roll back when methodA() commits, or methodB() may commit when methodA() is rolled back.  Therefore, this propagation can be used only for some separate part of work, not related to the outer transaction.  it can be used for writing log messages (it may commit if a transaction will roll back), or for caching, or for synchronization - for something where the failure of the part may not lead to the failure of the whole.
What is Mandatory transaction propagation and when it should be used	means that a method can be called only from the transactional method but doesn't start the transaction.  It may be used if the method needs a transaction, but it shouldn't be used independently, only as a part of some multi-step algorithm.  In case if the method marked as Mandatory is not running within a transaction, TransactionRequiredException is thrown. 
What is NotSupported transaction propagation and when it should be used	means that the method is not supporting transactions.  It should be run from non-transactional method, or if it is run from a transactional method, the transaction will be suspended.  It can be used only for reading data.  Because transactions involve overhead, this attribute may improve performance (avoid acquiring locks when reading data). 
What is Supports transaction propagation and when it should be used	means that method is not changing the behavior of a caller method: if the caller method was transactional, it will be executed within a transaction, otherwise, it will be executed without a transaction.  It can be used only for reading data, and most often in conjunction with readOnly=true.  In this case, we avoid the performance loss associated with creating a transaction and suspending a transaction, because all we need here is to read the data. NotSupported transaction propagation is used when you want to execute a piece of code without a transaction. If a transaction is available, it is suspended and the code is run without any transaction.
What is Never transaction propagation and when it should be used	Never means that a method should never be called from a transactional method.  If it was, we'd get IllegalTransactionStateException. It can be used for long-running methods which should never be executed inside a transaction because it may lead to low performance of database (locks will be held for a long time), and either not need transactions or use inner programmatic transactions.  It may also be used for testing purposes, to verify that a method is not transactional. 
How to rollback a transaction when using declarative transactions	By default, any RuntimeException|RollbackException leads the transaction to be rolled back.  Otherwise, it's possible to change a list of exceptions that may lead to rollback using the rollbackFor property of the @Transactional annotation.
What is the difference between logical and physical transactions	Physical transactions are actual database transactions, while logical transactions are @Transactional-annotated Spring methods.  Typically, a single physical transaction may include several logical transactions.
What happens with a physical transaction if the logical transaction is throwing an exception	If a @Transactional method throws an exception that is listed in rollbackFor (or any RuntimeException by default), the outer physical transaction will not commit and will throw UnexpectedRollbackException.  It will happen because throwing one of the exceptions from the rollbackFor list will mark the physical transaction as rollback-only, so it will not be able to commit even if the exception was caught. 
Is it possible to call a transactional method from the same class	No, this method will not be transactional in this case, because transactions are using the proxy mechanism, which can only be used only if Spring injects a reference to a proxied implementation, which adds transactional behavior.  You can use the self-injection pattern if you need to call a transactional method from the same class.
In which scenario optimistic or pessimistic locking should be used instead of transactions	Transactions are used for a short-running set of operations.  If we need some long-running operations, especially if a human is involved, we cannot do them in a transaction, because we shouldn't hold a connection and the locks for a long time.  We should use the approach named application transaction or long conversation.  A typical scenario involves a user who needs to change some data in the database: the user opens data in a form (reads data), then edits data in the form, then pushes the Save button to write the updated data back to the database.  In this scenario, we may face a lost update|unrepeatable read problem.  the user Alice and the user Bob are opening the same data, then the user Alice saves her changes, then the user Bob saves his changes.  In this scenario, Bob's changes will overwrite the changes made by Alice.  How to solve it? We may forbid to change data that was opened by someone else (it can be opened in read-only mode) - this option is named pessimistic locking.  Otherwise, we may allow everyone to change the data, but only changes made by the first user will be successfully saved - it is named optimistic locking (because we hope that the first user will be the only one).  Typically, optimistic locking is preferable for this scenario.
What is optimistic locking and how does it work	 is a strategy to prevent concurrent updates of the same data by multiple users without using locks.  To make it possible, the data entity has a special property (of type int/long/Date) marked with the @Version annotation.  Usually, it is a number initially set to 0. When the entity is read, the version is also read.  If anyone changes the data, it automatically increases a version.  If we need to save the updated data, we should first check if the version has been changed since we read the data (it is done automatically by Hibernate).  If it has not been changed, we can be sure that nobody else touched the data (represented as a database row) and we are working with this data exclusively.  Otherwise, if the version is different, it means that the data has been changed by somebody else, and we will get OptimisticLockException when trying to save this data.  In this case, we should roll back our change and re-read data from the database to get the actual data.  Then we can show it to the user or merge our changes with the updates from the database and try to save it again.  This strategy allows avoiding lost updates because changes of the same data made by another user will not be overwritten by us. Optimistic locking can improve performance in systems where conflicts between transactions are rare because it avoids the overhead of acquiring and releasing locks. However, if conflicts are frequent, optimistic locking can result in many transactions being rolled back and retried, reducing performance.
What is pessimistic locking and how does it work	is called "pessimistic" because the system assumes the worst that two or more users will want to update the same record at the same time, and therefore the system prevents that possibility by locking the record, no matter how unlikely conflicts are.  Pessimistic locking is used internally and automatically when we use transactions (depending on the isolation level).  However, it is possible to manually acquire the lock for the entity by using entityManager.lock(entity, PESSIMISTIC_WRITE), or the @Lock(PESSIMISTIC_WRITE) annotation for the method - in this case, we may acquire the lock at the moment of reading data.  It will execute a SELECT FOR UPDATE query, which will acquire the exclusive lock.  The lock will prevent changing data (because changing data needs to acquire an exclusive lock) and prevent acquiring the lock for this data by other transactions.  Otherwise, we can acquire PESSIMISTIC_READ (shared) lock which wouldn't allow changing data but will assure that anyone else wouldn't be able to get an exclusive lock to change the data.  If the database does not support shared locks (for example Oracle), then a shared lock request (PESSIMISTIC_READ) will simply acquire an exclusive lock request (PESSIMISTIC_WRITE).  Pessimistic locks are automatically released at the transaction end (on commit/rollback) and should be used only inside the transaction.
What is a distributed transaction and how does it work	A distributed transaction is a set of operations on the data that is performed across two or more databases (running on different servers).  It can be implemented by using a two-phase|one-phase commit scenario:  1) The transaction dispatcher starts the distributed transaction, and all the participants must vote that they have acquired all necessary locks and are ready to commit the transaction - this state is named transaction prepared.  2) If any participant of the distributed transaction couldn't prepare the transaction (wasn't able to acquire the necessary locks), the transaction is marked for rollback and all participants should rollback it.If all participants were able to prepare the transaction, the transaction dispatcher says that distributed transaction can be fixed. Then, the commit is executed for all participants of the distributed transaction.
How can a distributed transaction be used	Spring supports distributed transactions through JTA (Java Transaction API, part of JEE specifications), or by using the ChainedTransactionManager class built into Spring.  However, ChainedTransactionManager is not a reliable solution and has been deprecated.  You can use JTA with some JEE application server or a lightweight JTA provider like Atomikos/Bitronix/Narayana (if you don't want to use a JEE application server).  Otherwise, you can use the SAGA pattern.
What is the SAGA pattern and when it should be used	SAGA is the best option to implement distributed transactions, which may be used not just for relational databases, but also for NoSQL databases and various other data sources.  SAGA needs some messaging channel to exchange data between servers (it can be Kafka/Rabbit/JMS).  Every transactional operation A in SAGA should have its reverse counterpart (to perform the rollback of the operation) - operation A'.  if we put some money into the account (operation A), we should define the reverse operation to withdraw the same amount from the account (operation A'), which will rollback the first operation.  Let's assume that we have 2 services involved in a distributed transaction, and we need the first service to execute operation A, and the second service to execute operation B.  After operation A is successfully completed, the first service is sending a message to the second service to run operation B.  If operation B completes successfully, the service sends an approval message to the first service about the successful completion.  If operation B fails, the second service is sending a message to the first service to rollback operation A, and the first service performs operation A' which will rollback operation A.  The same will happen if the first service doesn't get an approval message from the second service for a long time (if the second service is down). 
What is the purpose of Spring Security?	Is a framework that focuses on providing authentication and authorization capabilities for Java applications. It aims to secure applications by controlling access to resources and protecting against common security threats. With Spring Security, developers can implement secure authentication mechanisms, handle user authorization, and protect sensitive data from unauthorized access. Offer a highly customizable way of implementing authentication, authorization, and protection against common attacks such as session fixation, clickjacking, and cross-site request forgery
Which of the following is used for storing user passwords securely?	user passwords are securely stored using hashing algorithms. Hashing algorithms convert passwords into irreversible, fixed-length strings, making it computationally infeasible to reverse-engineer the original password.
Which annotation is used to secure a method or a class with method-level security?	The @Secured annotation is used to secure a method or a class with method-level security. It allows you to specify the roles or authorities required to access the secured method or class.
What is the purpose of CSRF protection?	Is designed to protect against cross-site request forgery attacks. It ensures that requests made to the application originate from trusted sources, preventing malicious entities from tricking users into performing unintended actions.
Customization 	WebClient allows for easy customization through its Builder pattern. You can customize various aspects of the HTTP requests, such as setting headers, query parameters, timeouts, and authentication. You can also configure response handling, such as error handling and deserialization of response bodies. WebClient provides methods like defaultHeader, defaultHeaders, defaultRequest, defaultRequestHeaders, and exchangeStrategies to customize the client behavior according to your specific requirements. Additionally, you can extend WebClient and create your own custom WebClient instances with pre-configured settings or custom behavior.